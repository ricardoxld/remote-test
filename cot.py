from flask import jsonify, Flask
from flask_restful import Resource, Api, reqparse
import streamlit as st
from openai import OpenAI
import os
import json
import time
import requests



##https://zw.caseeder.cn/nlp/public/zkzw.do/downloadFile/%E5%B9%BF%E5%B7%9E%E5%B8%82%E4%BA%BA%E5%8A%9B%E8%B5%84%E6%BA%90%E5%92%8C%E7%A4%BE%E4%BC%9A%E4%BF%9D%E9%9A%9C%E5%B1%80%E5%85%B3%E4%BA%8E%E5%B9%BF%E5%B7%9E%E5%B8%822022%E5%B9%B4%E5%BA%A6%E7%A4%BE%E4%BC%9A%E4%BF%9D%E9%99%A9%E5%9F%BA%E9%87%91%E9%A2%84%E7%AE%97%E6%89%A7%E8%A1%8C%E6%83%85%E5%86%B5%E5%AE%A1%E8%AE%A1%E6%95%B4%E6%94%B9%E6%83%85%E5%86%B5%E7%9A%84%E5%85%AC%E5%91%8A.pdf?fileId=64df3da3-2da1-4635-8154-fb94647b4549&disposition=inline&
app = Flask(__name__)
api = Api(app)

def make_api_call(messages, is_final_answer=False):
    client = OpenAI(
        api_key = "sk-ftqwvyglkjlusikmrjylhyukwbmxjeoctskfriykqkttjbnf",
        base_url = "https://api.siliconflow.cn/v1",
    )

    for attempt in range(3): 
        try:
            response = client.chat.completions.create(
                model= "Qwen/Qwen2.5-7B-Instruct",  # Qwen/Qwen2.5-7B-Instruct meta-llama/Meta-Llama-3.1-8B-Instruct
                messages=messages,
                temperature=0.2
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            if attempt == 2:
                if is_final_answer:
                    return {"title": "Error",
                            "content": f"Failed to generate final answer after 3 attempts. Error: {str(e)}"}
                else:
                    return {"title": "Error", "content": f"Failed to generate step after 3 attempts. Error: {str(e)}",
                            "next_action": "final_answer"}
            time.sleep(1)  # Wait for 1 second before retrying

def generate_response(prompt):
    messages = [
        {"role": "system", "content": """ä½ æ˜¯ä¸€ä½å…·æœ‰é«˜çº§æ¨ç†èƒ½åŠ›çš„äººå·¥æ™ºèƒ½ä¸“å®¶åŠ©ç†ã€‚ä½ çš„ä»»åŠ¡æ˜¯å¯¹ä½ çš„æ€ç»´è¿‡ç¨‹è¿›è¡Œè¯¦ç»†çš„ã€å¾ªåºæ¸è¿›çš„è§£é‡Šã€‚å¯¹äºæ¯ä¸€æ­¥ï¼š
            1.æä¾›ä¸€ä¸ªæ¸…æ™°ã€ç®€æ´çš„æ ‡é¢˜ï¼Œæè¿°å½“å‰çš„æ¨ç†é˜¶æ®µã€‚
            2.å¯¹äºæ¯ä¸ªæ­¥éª¤ï¼Œæä¾›ä¸€ä¸ªæè¿°ä½ åœ¨è¯¥æ­¥éª¤ä¸­æ­£åœ¨åšä»€ä¹ˆçš„æ ‡é¢˜ï¼Œä»¥åŠè¯¦ç»†çš„å†…å®¹ã€‚
            3.å†³å®šæ˜¯å¦éœ€è¦å¦ä¸€ä¸ªæ­¥éª¤ï¼Œæˆ–è€…æ˜¯å¦å‡†å¤‡ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
            4.ä»¥JSONæ ¼å¼å“åº”ï¼ŒåŒ…å«'title'ï¼ˆæ ‡é¢˜ï¼‰ã€'content'ï¼ˆå†…å®¹ï¼‰å’Œ'next_action'ï¼ˆä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œå¯ä»¥æ˜¯'continue'æˆ–è€…'final_answer'ï¼‰é”®ã€‚
            å…³é”®è¯´æ˜ï¼š
            - å°½å¯èƒ½ä½¿ç”¨å¤šçš„æ¨ç†æ­¥éª¤ï¼Œè‡³å°‘3æ­¥ã€‚
    Â Â Â Â Â Â Â Â - æ„è¯†åˆ°ä½ ä½œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„å±€é™æ€§ï¼Œæ¸…æ¥šä½ èƒ½åšä»€ä¹ˆå’Œä¸èƒ½åšä»€ä¹ˆã€‚
    Â Â Â Â Â Â Â Â - åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œæ¢ç´¢æ›¿ä»£ç­”æ¡ˆã€‚
    Â Â Â Â Â Â Â Â - è€ƒè™‘åˆ°ä½ å¯èƒ½ä¼šå‡ºé”™ï¼Œå¹¶ä¸”å¦‚æœä½ çš„æ¨ç†æ˜¯é”™è¯¯çš„ï¼Œé”™è¯¯å¯èƒ½å‡ºç°åœ¨ä»€ä¹ˆåœ°æ–¹ã€‚
    Â Â Â Â Â Â Â Â - å…¨é¢æµ‹è¯•æ‰€æœ‰å…¶ä»–å¯èƒ½æ€§ã€‚ä½ å¯èƒ½æ˜¯é”™çš„ã€‚
    Â Â Â Â Â Â Â Â - å½“ä½ è¯´ä½ æ­£åœ¨é‡æ–°æ£€æŸ¥æ—¶ï¼ŒçœŸæ­£åœ°é‡æ–°æ£€æŸ¥ï¼Œå¹¶ä½¿ç”¨å¦ä¸€ç§æ–¹æ³•è¿›è¡Œæ£€æŸ¥ã€‚
    Â Â Â Â Â Â Â Â - ä¸è¦åªæ˜¯è¯´ä½ åœ¨é‡æ–°æ£€æŸ¥ã€‚ä½¿ç”¨è‡³å°‘3ç§æ–¹æ³•å¾—å‡ºç­”æ¡ˆã€‚é‡‡ç”¨æœ€ä½³å®è·µã€‚
            """
        },
        {"role": "user", "content": prompt},
        {"role": "assistant","content": "è°¢è°¢ï¼æˆ‘ç°åœ¨ä¼šæ ¹æ®æˆ‘çš„æŒ‡ç¤ºä¸€æ­¥æ­¥è¿›è¡Œæ€è€ƒï¼Œä»åˆ†è§£é—®é¢˜å¼€å§‹ï¼Œä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚"}
    ]

    steps = []
    step_count = 1
    total_thinking_time = 0

    while True:
        start_time = time.time()
        time.sleep(3) # Simulate thinking time
        step_data = make_api_call(messages, 1000)
        print(step_data)
        end_time = time.time()
        thinking_time = end_time - start_time
        total_thinking_time += thinking_time

        steps.append((f"æ­¥éª¤ {step_count}: {step_data['title']}", step_data['content'], thinking_time))

        messages.append({"role": "assistant", "content": json.dumps(step_data)})

        if step_data['next_action'] == 'final_answer':
            break

        step_count += 1

        # Yield after each step for Streamlit to update
        yield steps, None  # We're not yielding the total time until the end

    # Generate final answer
    messages.append({"role": "user", "content": "è¯·æ ¹æ®ä¸Šè¿°æ¨ç†æä¾›æœ€ç»ˆç­”æ¡ˆï¼Œä½¿ç”¨ä¸­æ–‡å›ç­”ã€‚"})

    start_time = time.time()
    final_data = make_api_call(messages, is_final_answer=True)
    end_time = time.time()
    thinking_time = end_time - start_time
    total_thinking_time += thinking_time

    steps_json = json.dumps(steps, ensure_ascii=False, indent=4)
    return steps_json

# å®šä¹‰è§£æå™¨
parser = reqparse.RequestParser()
parser.add_argument('url', type=str, required=True, location="json")

class CoT(Resource):
    def post(self):
        args = parser.parse_args()
        user_query = args['query']

        st.set_page_config(page_title="OpenAI Reasoning Chains", page_icon="ğŸ§ ", layout="wide")

        st.title("é€šä¹‰åƒé—®æ¨ç†é“¾")

        st.markdown("""
        è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨æç¤ºåˆ›å»ºé€šä¹‰åƒé—®æ¨ç†é“¾çš„æ—©æœŸåŸå‹ï¼Œæ—¨åœ¨æé«˜è¾“å‡ºçš„å‡†ç¡®æ€§ã€‚
        """)

        try:
            st.write("ç”Ÿæˆå“åº”ä¸­...")

            # Create empty elements to hold the generated text and total time
            response_container = st.empty()
            time_container = st.empty()

            result = generate_response(user_query)
            return result
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸å¤„ç†
            error_message = str(e)
            return jsonify({"error": error_message})

api.add_resource(CoT, '/cot')

if __name__ == '__main__':
    app.run(host="0.0.0.0", debug=False, port=15884)
